{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyYNDDSWtxij"
      },
      "source": [
        "Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7l6es42eti-a",
        "outputId": "c18d96a6-7045-47ff-c7de-ca76ae1faa1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6joFB82Zt2CC"
      },
      "source": [
        "Setup Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75h9wIsStw-K",
        "outputId": "273d17d0-b173-4aa5-be54-62073ebbe3b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.10/dist-packages (0.3.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from kagglehub) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kagglehub) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kagglehub) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub) (2024.12.14)\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/cookiefinder/tomato-disease-multiple-sources?dataset_version_number=1...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1.37G/1.37G [00:17<00:00, 82.3MB/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install kagglehub\n",
        "import kagglehub\n",
        "\n",
        "path = kagglehub.dataset_download(\"cookiefinder/tomato-disease-multiple-sources\")\n",
        "\n",
        "!mv /root/.cache/kagglehub/datasets/cookiefinder/tomato-disease-multiple-sources/versions/ /content/drive/MyDrive/tomato\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4ELlFAjuSUt"
      },
      "source": [
        "Set the dataset directories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "m6FYLT9OuVo9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "def check_and_remove_corrupted_images(directory):\n",
        "    for subdir, dirs, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            try:\n",
        "                img_path = os.path.join(subdir, file)\n",
        "                with Image.open(img_path) as img:\n",
        "                    img.verify()\n",
        "            except (IOError, SyntaxError) as e:\n",
        "                print(f\"Corrupted image: {img_path}\")\n",
        "                os.remove(img_path)\n",
        "\n",
        "train_dir = '/content/drive/MyDrive/tomato/1/train'\n",
        "valid_dir = '/content/drive/MyDrive/tomato/1/valid'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "RPrOu8I03p2J"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from PIL import ImageFile\n",
        "\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkg5f_vpuvX_"
      },
      "source": [
        "Data Generators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lv2CGrqpuyUw",
        "outputId": "6819a90c-2ae3-490b-8303-36cb12b436e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 25851 images belonging to 11 classes.\n",
            "Found 6683 images belonging to 11 classes.\n"
          ]
        }
      ],
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    zoom_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_data = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=8,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "valid_data = valid_datagen.flow_from_directory(\n",
        "    valid_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=8,\n",
        "    class_mode='categorical'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQcZmahwu0ef"
      },
      "source": [
        "Build the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NIEK6JHu21n"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
        "\n",
        "model = Sequential([\n",
        "    Input(shape=(150, 150, 3)),\n",
        "    Conv2D(32, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    GlobalAveragePooling2D(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(len(train_data.class_indices), activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWm4G18Hu5g_"
      },
      "source": [
        "Add Checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "3thEjobqu9Uo"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "checkpoint_path = '/content/drive/MyDrive/checkpoints/best_model.keras'\n",
        "checkpoint = ModelCheckpoint(checkpoint_path, save_best_only=True, monitor='val_loss', mode='min')\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KV1Irfqu_Gw"
      },
      "source": [
        "Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mssn0xNPvA6R",
        "outputId": "93fc151e-c6a4-4f53-ae2c-1b8ef31619f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m3232/3232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9216s\u001b[0m 3s/step - accuracy: 0.6390 - loss: 1.0290 - val_accuracy: 0.7432 - val_loss: 0.7229\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "model = load_model(checkpoint_path)\n",
        "\n",
        "\n",
        "last_completed_epoch = 4\n",
        "total_epochs = 5\n",
        "\n",
        "history = model.fit(\n",
        "    train_data,\n",
        "    validation_data=valid_data,\n",
        "    epochs=total_epochs,\n",
        "    initial_epoch=last_completed_epoch,\n",
        "    callbacks=[early_stopping, checkpoint]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fr9QtYC7vDeq"
      },
      "source": [
        "Resume Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tamvh-PIvN2L"
      },
      "source": [
        "Save Model and Logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "sb5GjiImvddk"
      },
      "outputs": [],
      "source": [
        "model.save('/content/drive/MyDrive/final_model.keras')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}